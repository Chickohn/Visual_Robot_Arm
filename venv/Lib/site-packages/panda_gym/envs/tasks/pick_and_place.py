from typing import Any, Dict

import numpy as np

from panda_gym.envs.core import Task
from panda_gym.pybullet import PyBullet
from panda_gym.utils import distance

class PickAndPlace(Task):
    def __init__(
        self,
        sim,
        get_ee_position,
        inverse_kinematics,
        gripper_width,
        reward_type="sparse",
        distance_threshold=0.01,
        goal_range=0.3,
        beaker_files_folder="Beaker_Files",
    ) -> None:
        super().__init__(sim)
        self.reward_type = reward_type
        self.distance_threshold = distance_threshold
        self.get_ee_position = get_ee_position
        self.inverse_kinematics = inverse_kinematics
        self.gripper_width = gripper_width
        self.goal_range = np.array([goal_range / 2, goal_range / 2, 0.0])
        self.beaker_files_folder = beaker_files_folder
        self.beaker_files = ["Beaker_500ml.obj"] # "beaker_250ml_inst.usda", "beaker_250ml.usda", "beaker_500ml_inst_mesh.usda", "beaker_500ml_inst.usda", "beaker_500ml.usda", "beaker_500ml.usd"]
        self.current_beaker_id = None
        self.prev_distance_to_goal = None
        self.current_subgoal = "above"
        self.win = False
        self.object_grabbed = False
        with self.sim.no_rendering():
            self._create_scene()

    def _create_scene(self) -> None:
        self.sim.create_plane(z_offset=-0.4)
        self.sim.create_table(length=1.1, width=0.7, height=0.4, x_offset=-0.3)
        self.sim.create_sphere(
            body_name="target",
            radius=0.01,
            mass=0.0,
            ghost=True,
            position=np.zeros(3),
            rgba_color=np.array([0.1, 0.9, 0.1, 0.3]),
        )
        self.sim.create_sphere(
            body_name="base",
            radius=0.005,
            mass=0.0,
            ghost=True,
            position=np.zeros(3),
            rgba_color=np.array([0.1, 0.1, 0.9, 0.3]),
        )
        self.sim.create_sphere(
            body_name="ee",
            radius=0.005,
            mass=0.0,
            ghost=True,
            position=np.zeros(3),
            rgba_color=np.array([0.9, 0.1, 0.1, 0.3]),
        )
        self.current_beaker_id = self.sim.loadURDF(fileName="Beaker_500ml.urdf", body_name="object")

    def get_obs(self) -> np.ndarray:
        object_position = self.sim.get_base_position("object")
        object_rotation = self.sim.get_base_rotation("object")
        object_velocity = self.sim.get_base_velocity("object")

        if self.current_subgoal == "above":
            self.sim.set_base_pose("target", object_position + np.array([0.0, 0.0, 0.09]), np.array([1.0, 0.0, 0.0, 1.0]))
        else:
            self.sim.set_base_pose("target", self.final_goal, np.array([1.0, 0.0, 0.0, 1.0]) )
        self.sim.set_base_pose("base", self.sim.get_base_position("object") + np.array([0.0, 0.0, 0.037]), np.array([1.0, 0.0, 0.0, 1.0]))
        self.sim.set_base_pose("ee", self.get_ee_position(), np.array([1.0, 0.0, 0.0, 1.0]))
        # # print("object position: ", object_position, "\n", object_rotation, "\n", object_velocity, "\n target position: ", self.sim.get_base_position("target")-object_position)
        
        object_angular_velocity = self.sim.get_base_angular_velocity("object")
        gripper_width = np.array([self.gripper_width(), self.gripper_width(), self.gripper_width()])
        gripper_position = self.get_ee_position()

        self.goal = self._sample_goal()

        observation = np.concatenate([object_position,
                                      object_rotation,
                                      object_velocity,
                                      object_angular_velocity,
                                      gripper_position,
                                      gripper_width])
        return observation

    def get_achieved_goal(self) -> np.ndarray:
        ee_position = self.get_ee_position()
        # object_position = np.array(self.sim.get_base_position("object"))
        return ee_position
    
    # def set_robot_position_closer_to_goal(self):
    #     target_position = np.array(self.sim.get_base_position("target"))
    
    #     random_distance = np.random.uniform(0.001, 0.15)
    #     end_effector_position = target_position + (np.array([0, 0, 0])*random_distance)
    #     joint_angles = self.inverse_kinematics(6, end_effector_position, np.array([0, 0, 0, 1]))
        
    #     # Set the robot's joint angles to the calculated angles
    #     # print(joint_angles)
    #     self.sim.set_joint_angles("panda", np.ndarray([0, 1, 2, 3, 4, 5, 6, 7, 8]), joint_angles)

    def reset(self) -> None:
        super().reset()
        self.win = False
        self.current_subgoal = "above"
        self.object_grabbed = False
        self.final_goal = self.np_random.uniform(
            self.sim.get_base_position("target") + np.array([-0.00, -0.00, 0.05]),
            self.sim.get_base_position("target") + np.array([0.00, 0.00, 0.05]))
        self.goal = self._sample_goal()
        object_position = self._sample_object()
        self.sim.set_base_pose("object", object_position, np.array([1.0, 0.0, 0.0, 1.0]))
        self.prev_distance_to_goal = np.linalg.norm(self.goal - self.get_achieved_goal())

    def _sample_goal(self) -> np.ndarray:
        """Randomize goal."""
        if self.current_subgoal == "above":
            goal = self.sim.get_base_position("target")
        elif self.current_subgoal == "pregrab" or self.current_subgoal == "grab":
            goal = self.sim.get_base_position("base")
        else:
            goal = self.final_goal
        return goal
    
    def _sample_object(self) -> np.ndarray:
        """Randomize start position of object."""
        object_position = np.array([0.0, 0.0, 0.0])
        noise = self.np_random.uniform(-self.goal_range, self.goal_range)
        object_position += noise
        return object_position
    
    def get_goal_position(self) -> np.ndarray:
        return self.goal
    
    def is_success(self, achieved_goal: np.ndarray, desired_goal: np.ndarray) -> np.ndarray:
        return np.array(self.win, dtype=bool)
    
    def _check_grabbed(self):
        # print(self.get_ee_position(), (self.sim.get_base_position("object")+np.array([0.0, 0.0, 0.037])))
        cup_distance = distance(self.get_ee_position(), (self.sim.get_base_position("object")+np.array([0.0, 0.0, 0.037])))
        # print("cup:", cup_distance)
        if abs(cup_distance) < 0.01:
            if self.gripper_width() < 0.0665:
                # print("grabbed")
                return True
        else:
            return False

    def compute_reward(self, achieved_goal, desired_goal, info: Dict[str, Any]) -> np.ndarray:
        # d = distance(achieved_goal, desired_goal)
        # if self.reward_type == "sparse":
        #     return -np.array(d > self.distance_threshold, dtype=np.float32)
        # else:
        #     return -d.astype(np.float32)
        
        # Possible other reward Strategy:
        d = distance(achieved_goal, desired_goal)

        # Sub-goal 1: Reaching the object
        if self.current_subgoal == "above":
            if (d < self.distance_threshold).all():
                reward = 1.0  # Reward for reaching the object
                print("above complete")
                print(self.gripper_width)
                if self.gripper_width() > 0.06:
                    reward = 1.5
                self.current_subgoal = "pregrab"  # Move to the next sub-goal
            else:
                reward = -1.0  # Penalize based on distance

        elif self.current_subgoal == "pregrab":
            if (d < self.distance_threshold).all():
                reward = 3.0
                print("pregrab complete")
                self.current_subgoal = "grab"
            else:
                reward = -1.0

        # Sub-goal 2: Grabbing the object
        elif self.current_subgoal == "grab":
            if self._check_grabbed():  # Assume this method checks if the object has been grabbed
                reward = 5.0  # Higher reward for grabbing
                print("grab complete")
                self.current_subgoal = "lift"
                self.object_grabbed = True
            else:
                reward = -1.0  # Continue to penalize based on distance to encourage grabbing

        # Sub-goal 3: Moving the object to the target location
        elif self.current_subgoal == "lift":
            od = distance(self.sim.get_base_position("base"), self.get_ee_position())
            if (od < self.distance_threshold*2).all():  # Assuming `d` is recalculated for object-to-goal distance
                if (d < self.distance_threshold).all():
                    print("=============lift complete=========")
                    self.win = True
                    reward = 10.0  # Highest reward for successful placement
                else:
                    reward = -1.0
            else:
                reward = -1.0 # -od  # Penalize heavily for dropping the beaker
        
        # print(self.current_subgoal, d)
        return np.array(reward, dtype=np.float32)

# class PickAndPlace(Task):
#     def __init__(
#         self,
#         sim: PyBullet,
#         reward_type: str = "sparse",
#         distance_threshold: float = 0.05,
#         goal_xy_range: float = 0.3,
#         goal_z_range: float = 0.2,
#         obj_xy_range: float = 0.3,
#     ) -> None:
#         super().__init__(sim)
#         self.reward_type = reward_type
#         self.distance_threshold = distance_threshold
#         self.object_size = 0.04
#         self.goal_range_low = np.array([-goal_xy_range / 2, -goal_xy_range / 2, 0])
#         self.goal_range_high = np.array([goal_xy_range / 2, goal_xy_range / 2, goal_z_range])
#         self.obj_range_low = np.array([-obj_xy_range / 2, -obj_xy_range / 2, 0])
#         self.obj_range_high = np.array([obj_xy_range / 2, obj_xy_range / 2, 0])
#         with self.sim.no_rendering():
#             self._create_scene()

#     def _create_scene(self) -> None:
#         """Create the scene."""
#         self.sim.create_plane(z_offset=-0.4)
#         self.sim.create_table(length=1.1, width=0.7, height=0.4, x_offset=-0.3)
#         self.sim.create_box(
#             body_name="object",
#             half_extents=np.ones(3) * self.object_size / 2,
#             mass=1.0,
#             position=np.array([0.0, 0.0, self.object_size / 2]),
#             rgba_color=np.array([0.1, 0.9, 0.1, 1.0]),
#         )
#         self.sim.create_box(
#             body_name="target",
#             half_extents=np.ones(3) * self.object_size / 2,
#             mass=0.0,
#             ghost=True,
#             position=np.array([0.0, 0.0, 0.05]),
#             rgba_color=np.array([0.1, 0.9, 0.1, 0.3]),
#         )

#     def get_obs(self) -> np.ndarray:
#         # position, rotation of the object
#         object_position = self.sim.get_base_position("object")
#         object_rotation = self.sim.get_base_rotation("object")
#         object_velocity = self.sim.get_base_velocity("object")
#         object_angular_velocity = self.sim.get_base_angular_velocity("object")
#         observation = np.concatenate([object_position, object_rotation, object_velocity, object_angular_velocity])
#         return observation

#     def get_achieved_goal(self) -> np.ndarray:
#         object_position = np.array(self.sim.get_base_position("object"))
#         return object_position

#     def reset(self) -> None:
#         self.goal = self._sample_goal()
#         object_position = self._sample_object()
#         self.sim.set_base_pose("object", object_position, np.array([0.0, 0.0, 0.0, 1.0]))

#     def _sample_goal(self) -> np.ndarray:
#         """Sample a goal."""
#         goal = np.array([0.0, 0.0, self.object_size / 2])  # z offset for the cube center
#         noise = self.np_random.uniform(self.goal_range_low, self.goal_range_high)
#         if self.np_random.random() < 0.3:
#             noise[2] = 0.0
#         goal += noise
#         return goal

#     def _sample_object(self) -> np.ndarray:
#         """Randomize start position of object."""
#         object_position = np.array([0.0, 0.0, self.object_size / 2])
#         noise = self.np_random.uniform(self.obj_range_low, self.obj_range_high)
#         object_position += noise
#         return object_position

#     def is_success(self, achieved_goal: np.ndarray, desired_goal: np.ndarray) -> np.ndarray:
#         d = distance(achieved_goal, desired_goal)
#         return np.array(d < self.distance_threshold, dtype=bool)

#     def compute_reward(self, achieved_goal, desired_goal, info: Dict[str, Any]) -> np.ndarray:
#         d = distance(achieved_goal, desired_goal)
#         if self.reward_type == "sparse":
#             return -np.array(d > self.distance_threshold, dtype=np.float32)
#         else:
#             return -d.astype(np.float32)
