from typing import Any, Dict

import numpy as np

from panda_gym.envs.core import Task
from panda_gym.pybullet import PyBullet
from panda_gym.utils import distance

class PickAndPlace(Task):
    def __init__(
        self,
        sim,
        get_ee_position,
        inverse_kinematics,
        gripper_width,
        reward_type="sparse",
        distance_threshold=0.01,
        goal_range=0.3,
        steps = 0,
        beaker_files_folder="Beaker_Files",
        initial_ee_position = None
    ) -> None:
        super().__init__(sim)
        self.reward_type = reward_type
        self.distance_threshold = distance_threshold
        self.get_ee_position = get_ee_position
        self.inverse_kinematics = inverse_kinematics
        self.gripper_width = gripper_width
        self.goal_range = np.array([goal_range / 2, goal_range / 2, 0.0])
        self.beaker_files_folder = beaker_files_folder
        self.beaker_files = ["Beaker_500ml.obj"] # "beaker_250ml_inst.usda", "beaker_250ml.usda", "beaker_500ml_inst_mesh.usda", "beaker_500ml_inst.usda", "beaker_500ml.usda", "beaker_500ml.usd"]
        self.current_beaker_id = None
        self.prev_distance_to_goal = None
        self.current_subgoal = "above"
        self.win = False
        self.object_grabbed = False
        self.initial_ee_position = initial_ee_position
        self.steps = steps
        self.subgoal_counters = {
            "above_complete": 0,
            "pregrab_complete": 0,
            "grab_complete": 0,
            "lift_complete": 0
        }
        self.subgoal_counters_pre_2000 = self.subgoal_counters
        with self.sim.no_rendering():
            self._create_scene()

    def _create_scene(self) -> None:
        self.sim.create_plane(z_offset=-0.4)
        self.sim.create_table(length=1.1, width=0.7, height=0.4, x_offset=-0.3)
        self.sim.create_sphere(
            body_name="target",
            radius=0.01,
            mass=0.0,
            ghost=True,
            position=np.zeros(3),
            rgba_color=np.array([0.0, 0.0, 0.0, 0.0]),
        )
        self.sim.create_sphere(
            body_name="base",
            radius=0.005,
            mass=0.0,
            ghost=True,
            position=np.zeros(3),
            # rgba_color=np.array([0.1, 0.1, 0.9, 0.3]),
        )
        self.sim.create_sphere(
            body_name="ee",
            radius=0.005,
            mass=0.0,
            ghost=True,
            position=np.zeros(3),
            # rgba_color=np.array([0.9, 0.1, 0.1, 0.3]),
        )
        self.current_beaker_id = self.sim.loadURDF(fileName="Beaker_500ml.urdf", body_name="object")

    def get_obs(self) -> np.ndarray:
        object_position = self.sim.get_base_position("object")
        object_rotation = self.sim.get_base_rotation("object")
        object_velocity = self.sim.get_base_velocity("object")

        if self.current_subgoal == "above":
            self.sim.set_base_pose("target", object_position + np.array([0.0, 0.0, 0.09]), np.array([1.0, 0.0, 0.0, 1.0]))
        else:
            self.sim.set_base_pose("target", self.final_goal, np.array([1.0, 0.0, 0.0, 1.0]) )
        self.sim.set_base_pose("base", self.sim.get_base_position("object") + np.array([0.0, 0.0, 0.037]), np.array([1.0, 0.0, 0.0, 1.0]))
        self.sim.set_base_pose("ee", self.get_ee_position(), np.array([1.0, 0.0, 0.0, 1.0]))
        object_angular_velocity = self.sim.get_base_angular_velocity("object")
        gripper_width = np.array([self.gripper_width(), self.gripper_width(), self.gripper_width()])
        gripper_position = self.get_ee_position()

        self.goal = self._sample_goal()

        self.steps += 1
        if self.steps % 2000 == 0:
            self.subgoal_counters_pre_2000 = {
                "above_complete": 0,
                "pregrab_complete": 0,
                "grab_complete": 0,
                "lift_complete": 0
            }
        else:
            self.subgoal_counters_pre_2000 = self.subgoal_counters
        # print(self.steps)

        observation = np.concatenate([object_position,
                                      object_rotation,
                                      object_velocity,
                                      object_angular_velocity,
                                      gripper_position,
                                      gripper_width])
        return observation

    def get_achieved_goal(self) -> np.ndarray:
        if self.current_subgoal == "lift":
            return self.sim.get_base_position("object")
        else:
            return self.get_ee_position()

    def reset(self) -> None:
        super().reset()
        self.win = False
        self.current_subgoal = "above"
        self.object_grabbed = False
        self.final_goal = self.np_random.uniform(
            self.sim.get_base_position("target") + np.array([-0.00, -0.00, 0.025]),
            self.sim.get_base_position("target") + np.array([0.00, 0.00, 0.025]))
        self.goal = self._sample_goal()
        object_position = self._sample_object()
        self.sim.set_base_pose("object", object_position, np.array([1.0, 1.0, 1.0, 1.0]))
        self.prev_distance_to_goal = distance(self.get_ee_position(), self.goal) # np.linalg.norm(self.goal - self.get_achieved_goal())
        self.initial_ee_position = None

    def _sample_goal(self) -> np.ndarray:
        """Randomize goal."""
        if self.current_subgoal == "above":
            goal = self.sim.get_base_position("target")
        elif self.current_subgoal == "pregrab" or self.current_subgoal == "grab":
            goal = self.sim.get_base_position("base")
        else:
            goal = self.final_goal
        return goal
    
    def _sample_object(self) -> np.ndarray:
        """Randomize start position of object."""
        object_position = np.array([0.0, 0.0, 0.0])
        noise = self.np_random.uniform(-self.goal_range, self.goal_range)
        object_position += noise
        return object_position
    
    def get_goal_position(self) -> np.ndarray:
        return self.goal
    
    def is_success(self, achieved_goal: np.ndarray, desired_goal: np.ndarray) -> np.ndarray:
        return np.array(self.win, dtype=bool)
    
    def _check_grabbed(self):
        # print(self.get_ee_position(), (self.sim.get_base_position("object")+np.array([0.0, 0.0, 0.037])))
        cup_distance = distance(self.get_ee_position(), (self.sim.get_base_position("object")+np.array([0.0, 0.0, 0.037])))
        # print("cup:", cup_distance)
        if abs(cup_distance) < 0.02:
            if self.gripper_width() < 0.0665:
                return True
        else:
            return False

    def compute_reward(self, achieved_goal, desired_goal, info: Dict[str, Any]) -> np.ndarray:
        # d = distance(achieved_goal, desired_goal)
        # if self.reward_type == "sparse":
        #     return -np.array(d > self.distance_threshold, dtype=np.float32)
        # else:
        #     return -d.astype(np.float32)
        
        # d = distance(self.get_ee_position(), self.goal)
        d = distance(achieved_goal, desired_goal)

        # Sub-goal 1: Reaching the object
        if self.current_subgoal == "above":
            if (d < (self.distance_threshold*2)).all():
                if self.gripper_width() > 0.06:
                    reward = 1.0 + self.gripper_width()  # Reward for reaching the object
                    self.subgoal_counters["above_complete"] += 1  # Increment counter
                    self.current_subgoal = "pregrab"
                    print("Above complete count:", self.subgoal_counters["above_complete"], "in", self.steps, "steps.")
                else:
                    reward = self.gripper_width()
                    print("Above but not open grip")
                # print("d:", d, "thresh:", self.distance_threshold)
                # reward = -np.array(d < self.distance_threshold, dtype=np.float32)
                # print("raw reward:", reward)
            else:
                reward = -d
                # if (d < self.prev_distance_to_goal).all():
                #     reward = 0.2-(d)  # This creates an exponential decay effect as d decreases
                # else:
                #     # Penalize exponentially less as the distance increases
                #     reward = -0.1*(d*10)
                # reward = np.clip(reward, -0.1, 0.1)
                # reward = -(d > self.prev_distance_to_goal).astype(np.float32) + 0.1
            # if self._check_grabbed():
            #     reward = 5.0
            #     print("grab complete")
            #     self.current_subgoal = "lift"
            #     self.object_grabbed = True


        elif self.current_subgoal == "pregrab":
            if (d < self.distance_threshold).all():
                reward = 5.0
                self.subgoal_counters["pregrab_complete"] += 1  # Increment counter
                print("Pregrab complete count:", self.subgoal_counters["pregrab_complete"])
                self.current_subgoal = "grab"
            else:
                reward = 3.0 -d + self.gripper_width()
            # if self._check_grabbed():
            #     reward = 5.0
            #     print("grab complete")
            #     self.current_subgoal = "lift"
            #     self.object_grabbed = True

        # Sub-goal 2: Grabbing the object
        elif self.current_subgoal == "grab":
            # Check if EE has moved from its initial position when subgoal was set
            current_ee_position = self.get_ee_position()
            if self.initial_ee_position is None:
                self.initial_ee_position = current_ee_position

            # Calculate the distance moved by the EE since the grab subgoal was set
            ee_movement = distance(current_ee_position, self.initial_ee_position)

            # Check if the object has been grabbed
            if self._check_grabbed():
                reward = 7.0  # High reward for successful grabbing
                self.subgoal_counters["grab_complete"] += 1  # Increment counter
                print("Grab complete count:", self.subgoal_counters["grab_complete"])
                self.current_subgoal = "lift"
                self.object_grabbed = True
                self.initial_ee_position = None  # Reset for next use
            else:
                reward = 5.0-self.gripper_width()  # Continue to penalize based on distance to encourage correct positioning
                
            # Severely penalize any movement of the EE from its position at the start of the grab phase
            if ee_movement > 0.01:  # Allow a tiny tolerance for imprecision
                reward -= 5 * ee_movement  # Apply heavy penalties for moving the EE


        # Sub-goal 3: Moving the object to the target location
        elif self.current_subgoal == "lift":
            print("lifting")
            od = distance(self.sim.get_base_position("base"), self.get_ee_position())
            if (od < self.distance_threshold*5).all():  # Assuming `d` is recalculated for object-to-goal distance
                if (d < self.distance_threshold*5).all():
                    print("=============lift complete=========")
                    self.subgoal_counters["lift_complete"] += 1  # Increment counter
                    print("Lift complete count:", self.subgoal_counters["lift_complete"])
                    self.win = True
                    reward = 10.0  # Highest reward for successful placement
                else:
                    reward = od
            else:
                reward = -od # Penalize heavily for dropping the beaker

        beaker_orientation = abs(self.sim.get_base_orientation("object") - [0.5, 0.5, 0.5, 0.5])
        if any(beaker_orientation > [0.1, 0.1, 0.1, 0.1]) and self.current_subgoal != "lift":
            reward += -sum(beaker_orientation)

        # import time
        # time.sleep(0.05)
        # print(self.current_subgoal, d)
        # print("reward:", reward)
        self.prev_distance_to_goal = d
        if self.steps % 12500 == 0:
            print(self.subgoal_counters_pre_2000)

        return np.array(reward, dtype=np.float32)





    # def compute_reward(self, achieved_goal, desired_goal, info: Dict[str, Any]) -> np.ndarray:
    #     d = distance(achieved_goal, desired_goal)

    #     if self.current_subgoal == "above":
    #         if (d < (self.distance_threshold*2)).all() and self.gripper_width() > 0.06:
    #             reward = 0.3-d  # Highest reward for the first critical step
    #             print(reward)
    #             self.current_subgoal = "pregrab"
    #             print("above")
    #         else:
    #             reward = -0.5-d

    #     elif self.current_subgoal == "pregrab":
    #         if (d < self.distance_threshold).all():
    #             reward = 0.5-d  # Slightly less reward, still significant
    #             self.current_subgoal = "grab"
    #         else:
    #             reward = -0.4-d

    #     elif self.current_subgoal == "grab":
    #         if self._check_grabbed():
    #             reward = 0.7-d  # Further reduced reward
    #             self.current_subgoal = "lift"
    #         else:
    #             reward = -0.3-d

    #     elif self.current_subgoal == "lift":
    #         if (d < self.distance_threshold).all():
    #             reward = 1.0-d  # Lowest reward for the final step
    #         else:
    #             reward = -0.2-d

    #     beaker_orientation = abs(self.sim.get_base_orientation("object") - [0.5, 0.5, 0.5, 0.5])
    #     if any(beaker_orientation > [0.1, 0.1, 0.1, 0.1]):
    #         reward += -sum(beaker_orientation)
        
    #     return np.array(reward, dtype=np.float32)